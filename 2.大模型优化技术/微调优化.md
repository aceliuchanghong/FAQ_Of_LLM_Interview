1. ğŸ’¡å¦‚æœæƒ³è¦åœ¨æŸä¸ªæ¨¡å‹åŸºç¡€ä¸Šåšå…¨å‚æ•°å¾®è°ƒï¼Œç©¶ç«Ÿéœ€è¦å¤šå°‘æ˜¾å­˜ï¼Ÿ

```text
è¦ç¡®å®šå…¨å‚æ•°å¾®è°ƒæ‰€éœ€çš„æ˜¾å­˜é‡ï¼Œéœ€è¦è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªå› ç´ ï¼š
1.æ¨¡å‹çš„å¤§å°ï¼šæ¨¡å‹çš„å¤§å°æ˜¯æŒ‡æ¨¡å‹å‚æ•°çš„æ•°é‡ã€‚
2.æ‰¹é‡å¤§å°ï¼šæ‰¹é‡å¤§å°æ˜¯æŒ‡åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ä¸€æ¬¡æ€§è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„æ ·æœ¬æ•°é‡ã€‚è¾ƒå¤§çš„æ‰¹é‡å¤§å°å¯ä»¥æé«˜è®­ç»ƒçš„æ•ˆç‡ï¼Œä½†ä¹Ÿéœ€è¦æ›´å¤šçš„æ˜¾å­˜
3.è®­ç»ƒæ•°æ®çš„ç»´åº¦ï¼šè®­ç»ƒæ•°æ®çš„ç»´åº¦æ˜¯æŒ‡è¾“å…¥æ•°æ®çš„å½¢çŠ¶ã€‚å¦‚æœè¾“å…¥æ•°æ®å…·æœ‰è¾ƒé«˜çš„ç»´åº¦ï¼Œä¾‹å¦‚å›¾åƒæ•°æ®ï¼Œé‚£ä¹ˆæ‰€éœ€çš„æ˜¾å­˜é‡å¯èƒ½ä¼šæ›´å¤§
4.è®­ç»ƒè®¾å¤‡çš„æ˜¾å­˜é™åˆ¶ï¼šæœ€åï¼Œéœ€è¦è€ƒè™‘è®­ç»ƒè®¾å¤‡çš„æ˜¾å­˜é™åˆ¶ã€‚
```

2. ğŸ’¡ä¸ºä»€ä¹ˆSFTä¹‹åæ„Ÿè§‰LLMå‚»äº†

```text
Supervised Fine-Tuningï¼ˆSFTï¼‰æœ‰ç›‘ç£å¾®è°ƒ
1.æ•°æ®åç§»ï¼šSFTè¿‡ç¨‹ä¸­ä½¿ç”¨çš„å¾®è°ƒæ•°æ®é›†å¯èƒ½ä¸åŸºåº§æ¨¡å‹åœ¨é¢„è®­ç»ƒé˜¶æ®µæ¥è§¦åˆ°çš„æ•°æ®åˆ†å¸ƒæœ‰æ‰€ä¸åŒã€‚
2.è¿‡æ‹Ÿåˆï¼šå¦‚æœå¾®è°ƒæ•°æ®é›†ç›¸å¯¹è¾ƒå°ï¼Œæˆ–è€…æ¨¡å‹çš„å®¹é‡ï¼ˆå‚æ•°æ•°é‡ï¼‰è¾ƒå¤§ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¿‡æ‹Ÿåˆå¾®è°ƒæ•°æ®
```

3. ğŸ’¡SFT å¾®è°ƒæ•°æ® å¦‚ä½•æ„å»º?

```text
æ”¶é›†åŸå§‹æ•°æ®=>æ ‡æ³¨æ•°æ®=>åˆ’åˆ†æ•°æ®é›†=>æ•°æ®é¢„å¤„ç†=>æ ¼å¼è½¬æ¢=>æ¨¡å‹å¾®è°ƒ=>æ¨¡å‹è¯„ä¼°
æ•°æ®è½¬æ¢:ä¸ºé€‚åˆæ¨¡å‹è®­ç»ƒçš„æ ¼å¼ã€‚å°†æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æ–‡ä»¶ã€JSONæ ¼å¼æˆ–å…¶ä»–é€‚åˆæ¨¡å‹è¾“å…¥çš„æ ¼å¼
```

4. ğŸ’¡è¿›è¡ŒSFTæ“ä½œçš„æ—¶å€™ï¼ŒåŸºåº§æ¨¡å‹é€‰ç”¨Chatè¿˜æ˜¯Base?

```text
åœ¨è¿›è¡ŒSupervised Fine-Tuningï¼ˆSFTï¼‰æ“ä½œæ—¶ï¼ŒåŸºåº§æ¨¡å‹çš„é€‰æ‹©ä¹Ÿå¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µæ¥å†³å®š
å¦‚æœç›‘ç£ä»»åŠ¡æ˜¯å¯¹è¯ç”Ÿæˆç›¸å…³çš„ï¼Œæ¯”å¦‚ç”Ÿæˆå¯¹è¯å›å¤æˆ–å¯¹è¯æƒ…æ„Ÿåˆ†ç±»ç­‰ï¼Œé‚£ä¹ˆé€‰æ‹©Chatæ¨¡å‹ä½œä¸ºåŸºåº§æ¨¡å‹å¯èƒ½æ›´åˆé€‚
å¦‚æœç›‘ç£ä»»åŠ¡æ˜¯å•è½®æ–‡æœ¬ç”Ÿæˆæˆ–éå¯¹è¯ç”Ÿæˆä»»åŠ¡ï¼Œé‚£ä¹ˆé€‰æ‹©Baseæ¨¡å‹ä½œä¸ºåŸºåº§æ¨¡å‹å¯èƒ½æ›´åˆé€‚ã€‚
```

5. ğŸ’¡è®­ç»ƒä¸­æ–‡å¤§æ¨¡å‹æœ‰å•¥ç»éªŒ

```text
1. æ•°æ®é¢„å¤„ç†ï¼šå¯¹äºä¸­æ–‡æ–‡æœ¬ï¼Œå¸¸è§çš„é¢„å¤„ç†æ­¥éª¤åŒ…æ‹¬åˆ†è¯ã€å»é™¤åœç”¨è¯ã€è¯æ€§æ ‡æ³¨ã€æ‹¼éŸ³è½¬æ¢ç­‰ã€‚åˆ†è¯æ˜¯ä¸­æ–‡å¤„ç†çš„åŸºæœ¬æ­¥éª¤
2. æ•°æ®å¢å¼ºï¼šä¸­æ–‡æ•°æ®é›†å¯èƒ½ç›¸å¯¹æœ‰é™ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯æ¥æ‰©å……æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨åŒä¹‰è¯æ›¿æ¢ã€éšæœºæ’å…¥æˆ–åˆ é™¤è¯è¯­ã€å¥å­é‡ç»„ç­‰æ–¹æ³•æ¥ç”Ÿæˆæ–°çš„è®­ç»ƒæ ·æœ¬ã€‚
3. å­—è¯çº§åˆ«çš„è¡¨ç¤ºï¼šä¸­æ–‡ä¸­æ—¢æœ‰å­—çº§åˆ«çš„è¡¨ç¤ºï¼Œä¹Ÿæœ‰è¯çº§åˆ«çš„è¡¨ç¤ºã€‚å¯¹äºå­—çº§åˆ«çš„è¡¨ç¤ºï¼Œå¯ä»¥ä½¿ç”¨å­—ç¬¦åµŒå…¥æˆ–è€…å­—çº§åˆ«çš„CNNã€RNNç­‰æ¨¡å‹ã€‚å¯¹äºè¯çº§åˆ«çš„è¡¨ç¤ºï¼Œå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡ï¼Œå¦‚Word2Vecã€GloVeç­‰ã€‚
4. é¢„è®­ç»ƒæ¨¡å‹ï¼šå¯ä»¥è€ƒè™‘ä½¿ç”¨å·²ç»åœ¨å¤§è§„æ¨¡ä¸­æ–‡è¯­æ–™ä¸Šé¢„è®­ç»ƒå¥½çš„æ¨¡å‹ä½œä¸ºåˆå§‹æ¨¡å‹ï¼Œç„¶ååœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨BERTã€GPTç­‰é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚è¿™æ ·å¯ä»¥åˆ©ç”¨å¤§è§„æ¨¡ä¸­æ–‡è¯­æ–™çš„ä¿¡æ¯ï¼Œæå‡æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚
5. ä¸­æ–‡ç‰¹å®šçš„ä»»åŠ¡ï¼šå¯¹äºä¸€äº›ä¸­æ–‡ç‰¹å®šçš„ä»»åŠ¡ï¼Œä¾‹å¦‚ä¸­æ–‡åˆ†è¯ã€å‘½åå®ä½“è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æç­‰ï¼Œå¯ä»¥ä½¿ç”¨ä¸€äº›ä¸­æ–‡ç‰¹å®šçš„å·¥å…·æˆ–è€…æ¨¡å‹æ¥è¾…åŠ©è®­ç»ƒã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨THULACã€LTPç­‰ä¸­æ–‡NLPå·¥å…·åŒ…ã€‚
6. è®¡ç®—èµ„æºï¼šè®­ç»ƒå¤§æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼ŒåŒ…æ‹¬GPUã€å†…å­˜å’Œå­˜å‚¨ã€‚å¯ä»¥è€ƒè™‘ä½¿ç”¨äº‘è®¡ç®—å¹³å°æˆ–è€…åˆ†å¸ƒå¼è®­ç»ƒæ¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚
7. è¶…å‚æ•°è°ƒä¼˜ï¼šå¯¹äºå¤§æ¨¡å‹çš„è®­ç»ƒï¼Œè¶…å‚æ•°çš„é€‰æ‹©å’Œè°ƒä¼˜éå¸¸é‡è¦ã€‚å¯ä»¥ä½¿ç”¨ç½‘æ ¼æœç´¢ã€éšæœºæœç´¢æˆ–è€…åŸºäºä¼˜åŒ–ç®—æ³•çš„è‡ªåŠ¨è°ƒå‚æ–¹æ³•æ¥å¯»æ‰¾æœ€ä½³çš„è¶…å‚æ•°ç»„åˆã€‚
```

6. ğŸ’¡LoRA(low-rank adaptation)  of large language models

[LoRA..ETC.md](..%2Flangchain%2FLoRA..ETC.md)

7. ä¸ºä»€ä¹ˆéœ€è¦ PEFTï¼Ÿ å‚æ•°é«˜æ•ˆå¾®è°ƒ

```text
PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰æ˜¯ä¸€ç§ç”¨äºå¾®è°ƒä»»åŠ¡çš„æ€§èƒ½ä¼°è®¡å’Œå»ºæ¨¡æ–¹æ³•ã€‚Transformerçš„ PEFT æ–¹æ³•
å®ƒçš„ä¸»è¦ç›®çš„æ˜¯å¸®åŠ©ç ”ç©¶äººå‘˜å’Œä»ä¸šè€…åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ›´å¥½åœ°ç†è§£å’Œé¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è¿›è¡Œæ›´æœ‰æ•ˆçš„æ¨¡å‹é€‰æ‹©å’Œè°ƒä¼˜ã€‚

```

8. PEFT(å‚æ•°é«˜æ•ˆå¾®è°ƒ) å’Œ å…¨é‡å¾®è°ƒåŒºåˆ«ï¼Ÿ

```text
PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰å’Œå…¨é‡å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰æ˜¯ä¸¤ç§ä¸åŒçš„å¾®è°ƒæ–¹æ³•
```

9. æœ‰å“ªäº›å¾®è°ƒ?

```text
Adapter Tuningã€LORAã€Prefix-Tuningã€Prompt Tuningã€P-tuningã€P-tuning v2
```

![img_8.png](..%2Fusing_files%2Fimg%2Flora%2Fimg_8.png)

![img_9.png](..%2Fusing_files%2Fimg%2Flora%2Fimg_9.png)


```python
#Adding the adapters in the layers
model = prepare_model_for_kbit_training(model)
peft_config = LoraConfig(
        r=16,
        lora_alpha=16,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj","gate_proj"]
    )
model = get_peft_model(model, peft_config)
# Hyperparamter
training_arguments = TrainingArguments(
    output_dir= "./results",
    num_train_epochs= 2,
    per_device_train_batch_size= 8,
    gradient_accumulation_steps= 2,
    optim = "paged_adamw_8bit",
    save_steps= 1000,
    logging_steps= 30,
    learning_rate= 2e-4,
    weight_decay= 0.001,
    fp16= False,
    bf16= False,
    max_grad_norm= 0.3,
    max_steps= -1,
    warmup_ratio= 0.3,
    group_by_length= True,
    lr_scheduler_type= "constant",
    report_to="wandb"
)
# Setting sft parameters
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    max_seq_length= None,
    dataset_text_field="chat_sample",
    tokenizer=tokenizer,
    args=training_arguments,
    packing= False,
)
# Save the fine-tuned model
trainer.model.save_pretrained(new_model)
wandb.finish()
model.config.use_cache = True
model.eval()
```

### Reference(å‚è€ƒæ–‡æ¡£)

* [å¾®è°ƒæ­¥éª¤](https://github.com/liguodongiot/llm-action)
* [è®°å¾—çœ‹ä¸€ä¸‹è¿™ä¸ª:mistral-7bå¾®è°ƒåˆ†æ­¥æŒ‡å—](https://gathnex.medium.com/mistral-7b-fine-tuning-a-step-by-step-guide-52122cdbeca8)

