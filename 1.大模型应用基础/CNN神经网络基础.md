## 卷积神经网络(Convolutional Neural Networks, CNN)
### What
```text
卷积是微积分里的一个术语，指卷积运算
Input是输入图片的像素矩阵，kernel是卷积核，选取一个滑动窗口，大小和卷积核一样，滑动窗口和卷积核进行点乘，得到的和作为输出像素值。
CNN网络里的卷积核类似图像处理的卷积核(滤波器)，图像处理中的滤波器是事先定义好的，每个滤波器都有特定的功能(图像锐化、检测边缘等)。
CNN网络利用神经网络的反向传播功能，通过损失函数来更新滤波器的值，使得滤波器能区分不同输入图像的共同点和不同点，对输入图片进行分类。

CNN网络结构一般包含卷积层、池化层、全连接层、激活函数、反向传播。

最常见的CNN网络结构如下：
INPUT -> [CONV -> RELU]*N -> [POOL?]*M -> [FC -> RELU]*K -> FC

eg:
INPUT -> FC,实现一个线性分类器，此处N = M = K = 0。
INPUT -> CONV -> RELU -> FC
INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC。此处在每个池化层之间有一个卷积层。
INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC。
(此处每个池化层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的池化操作前，
多重的卷积层可以从输入数据中学习到更多的复杂特征。)

总结
1：输入通道个数(如果是图片,那么rgb的,一般包含三个通道) 等于 卷积核通道个数(由输入矩阵的通道数所决定)
2：卷积核个数 等于 输出通道个数(一个卷积核其实就是一个filer)
最后训练的其实就是卷积核
```

![img_3.png](..%2Fusing_files%2Fimg%2FPyTorch%2Fimg_3.png)

![img_4.png](..%2Fusing_files%2Fimg%2FPyTorch%2Fimg_4.png)


```text
当我们谈论卷积运算时，可以将其比喻为一种“信息交互”的过程，就像两个人在沟通交流时相互影响、相互作用一样。
想象一下，有两个人，分别代表着两个信号或者两个函数。一个人手里拿着一张卡片，卡片上写着某种信息，比如数字或者一段文字，这代表一个信号或者函数。
另一个人也拿着一张卡片，上面也写着信息，代表另一个信号或者函数。
这两个人站在一起，他们开始交谈，每个人都在关注对方手上的卡片上的信息。然后，他们开始进行一种特殊的交互：
每个人都将自己手上卡片上的信息与对方手上的卡片上的信息进行一种“混合”。
这种混合的方式是，一个人的卡片上的信息按照一定的规则“滑动”或“移动”，然后将两个卡片上的信息进行一一对应相乘，并将结果相加起来这就是卷积运算的过程。
在这个过程中，每个人都在参与，他们的信息相互影响、相互作用，最终得到的结果就是卷积运算的输出。
```

### How

![img_1.png](..%2Fusing_files%2Fimg%2FPyTorch%2Fimg_1.png)

![img_2.png](..%2Fusing_files%2Fimg%2FPyTorch%2Fimg_2.png)

1：单通道卷积
```text
以单通道卷积为例，输入为（1,5,5），分别表示1个通道，宽为5，高为5。
假设卷积核大小为3x3，padding=0，stride=1。
卷积过程如下1：
相应的卷积核不断的在图像上进行遍历，最后得到3x3的卷积结果，结果如下2：
```

![img_2.png](..%2Fusing_files%2Fimg%2FCNN%2Fimg_2.png)

![img_3.png](..%2Fusing_files%2Fimg%2FCNN%2Fimg_3.png)


2.多通道卷积1
```text
以彩色图像为例，包含三个通道，分别表示RGB三原色的像素值，输入为（3,5,5），
分别表示3个通道，每个通道的宽为5，高为5。
假设卷积核只有1个，卷积核通道为3，每个通道的卷积核大小仍为3x3，padding=0，stride=1。

卷积过程如下，每一个通道的像素值与对应的卷积核通道的数值进行卷积，
因此每一个通道会对应一个输出卷积结果，三个卷积结果对应位置累加求和，
得到最终的卷积结果（这里卷积输出结果通道只有1个，因为卷积核只有1个）

可以这么理解：最终得到的卷积结果是原始图像各个通道上的综合信息结果。
```

![img_4.png](..%2Fusing_files%2Fimg%2FCNN%2Fimg_4.png)

```text
上述过程中，每一个卷积核的通道数量，必须要求与输入通道数量一致，
因为要对每一个通道的像素值要进行卷积运算，所以每一个卷积核的通道数量必须要与输入通道数量保持一致

我们把上述图像通道如果放在一块，计算原理过程还是与上面一样，堆叠后的表示如下：
```

![img_5.png](..%2Fusing_files%2Fimg%2FCNN%2Fimg_5.png)

3.多通道卷积2
```text
在上面的多通道卷积1中，输出的卷积结果只有1个通道，把整个卷积的整个过程抽象表示，过程如下：
```

![img_6.png](..%2Fusing_files%2Fimg%2FCNN%2Fimg_6.png)

```text
即：由于只有一个卷积核，因此卷积后只输出单通道的卷积结果
（黄色的块状部分表示一个卷积核，黄色块状是由三个通道堆叠在一起表示的，每一个黄色通道与输入卷积通道分别进行卷积，
也就是channel数量要保持一致，图片组这里只是堆叠放在一起表示而已）。

那么，如果要卷积后也输出多通道，增加卷积核（filers）的数量即可，示意图如下：
注:下面的feature map的颜色，只是为了表示不同的卷积核对应的输出通道结果，不是表示对应的输出颜色
```

![img_7.png](..%2Fusing_files%2Fimg%2FCNN%2Fimg_7.png)

```python
# 代码实现
import torch
 
in_channels = 5  #输入通道数量
out_channels =10 #输出通道数量
width = 100      #每个输入通道上的卷积尺寸的宽
heigth = 100     #每个输入通道上的卷积尺寸的高
kernel_size = 3  #每个输入通道上的卷积尺寸
batch_size = 1   #批数量
 
input = torch.randn(batch_size,in_channels,width,heigth)
conv_layer = torch.nn.Conv2d(in_channels,out_channels,kernel_size=kernel_size)
 
out_put = conv_layer(input)
 
print(input.shape)
print(out_put.shape)
print(conv_layer.weight.shape)
```


![img_8.png](..%2Fusing_files%2Fimg%2FCNN%2Fimg_8.png)


### 卷积层(Convolutional Layer)
```text
CNN网络在前向传播的时候，让每个滤波器(卷积核)都在输入数据的宽度和高度上滑动（更精确地说是卷积），
然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，滤波器与滑动窗口进行点乘，
点乘结果经过激活函数(常用Relu)之后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。
网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，
甚至可以是网络更高层上的蜂巢状或者车轮状图案。
卷积核直观上可以理解为具有学习功能的特征提取工具，提取目标物体的突出特征。

一般输入图像表示为(w x h x c)的矩阵，w和h表示输入图像的宽和高，c表示输入图像维度，
如果是灰度图像，则c=1，如果是RGB图像，则c=3。
在CNN网络里，输入图像一般有两种处理方式，一种是全转换为RGB图像，
如果是灰度图像，则将二维矩阵(w x h)复制3次，变成(w x h x 3)；
另一种是全转换为二维的灰度图像，像素值在0～255之间。
```
1. 卷积层参数
```text
1.步长(stride):
卷积核(滤波器)在输入数据上进行滑动时的间隔像素。如果滤波器的步长大于1，会使输出数据的尺寸小于输入数据
2.零填充（zero-padding）:
用0填充输入数据的边缘，0填充可以使输出数据和输入数据尺寸相同
3.感受野（receptive field）:
卷积核的空间尺寸，一般为3x3，5x5，7x7；感受野的尺寸(宽和高)是超参数，由用户自己定义，但是深度必须和输入数据的深度相等

注意：我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。
例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，
那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。
注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。
例2：假设输入数据体的尺寸是[16x16x20]，感受野尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20=180个连接。
再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）

4.输出通道数(channel):
通道(channel)对输入数据是指数据的深度，
比如RBG图像，channel是3；对输出数据是指卷积核的数量(不是深度，卷积核的深度与输入数据的深度保持一致)，
如下图，有 6×6×3 的图片样本，使用 3x3 尺寸的卷积核（filter）进行卷积操作，那么输入图片的 channels 为 3，
而卷积核的深度为3(与输入图片的深度保持一致)，所以卷积核是 3x3x3，如果只有1个卷积核，步长为1，不进行零填充。
那么每一层卷积核中的27个数字与分别与每一层样本对应相乘后得到一个和，一共有3层，对这3层的卷积结果再进行求和，得到第一个结果。
依次进行，最终得到 4×4的结果。如果卷积核个数为2，即 2x 3x3x3，那么结果为 2x4x4。
```
![img_6.png](..%2Fusing_files%2Fimg%2FPyTorch%2Fimg_6.png)

![img_5.png](..%2Fusing_files%2Fimg%2FPyTorch%2Fimg_5.png)

2. 卷积层的计算细节

如图，输入图像IN是RGB图像，kernel大小是3x3x3，kernel数量(输出通道数)为5
![img_7.png](..%2Fusing_files%2Fimg%2FPyTorch%2Fimg_7.png)
输入矩阵格式：四个维度，依次为：样本数、图像高度、图像宽度、图像通道数

输出矩阵格式：与输出矩阵的维度顺序和含义相同，但是后三个维度（图像高度、图像宽度、图像通道数）的尺寸发生变化。

权重矩阵（卷积核）格式：同样是四个维度，但维度的含义与上面两者都不同，为：卷积核高度、卷积核宽度、输入通道数、输出通道数（卷积核个数）

输入矩阵、权重矩阵、输出矩阵这三者之间的相互决定关系

卷积核的输入通道数（in depth）由输入矩阵的通道数所决定。（红色标注）

输出矩阵的通道数（out depth）由卷积核的输出通道数所决定。（绿色标注）

输出矩阵的高度和宽度（height, width）这两个维度的尺寸由输入矩阵、卷积核、扫描方式所共同决定。

下图输入图像是RGB图像，大小是 7x7x3 (用0填充了一圈)，卷积核大小 3x3x3，用了2个卷积核，步长为2，输出结果为 3x3x2，
下图在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来。

![convdemo.gif](..%2Fusing_files%2Fimg%2FPyTorch%2Fconvdemo.gif)

3. 卷积层特点
```text
1.大量的计算，网络的主要计算都产生在卷积层。
2.具有平移不变性，对待检测物体进行平移，不影响检测效果
3.参数共享，控制参数的数量。
4.卷积操作通常后面接的是ReLU层，对激活图中的每个元素做激活函数运算。
```

### 池化层(Pooling Layer)
```text
通常每个卷积层之后会紧跟Relu，激活需要学习的特征，连续的卷积层(包含Relu)之后，会插入一个池化层，
池化层的作用是降维，主要是降低输出数据的空间尺寸(不改变深度)，这样也能减少网络的参数量，也能有效控制过拟合。

不使用池化层：池化层不是必须的，目前有一些方法可以替代池化层，比如卷积层中使用更大的步长来降低数据体的尺寸。
```
![img.png](..%2Fusing_files%2Fimg%2FCNN%2Fimg.png)


### 全连接层(Fully-Connected Layer)

![img_1.png](..%2Fusing_files%2Fimg%2FCNN%2Fimg_1.png)

```text
全连接层的作用是将卷积提取的特征映射到每一类，方便损失函数打分。
全连接层的输入是前一层的所有神经元个数，输出是用户自定义，
fc = nn.Linear(in_features=input_size, out_features=50)
一般最后一层全连接层的输出个数是需要检测物体的类别数(对于分类网络)。

全连接层与卷积层的区别?
1.卷积层的输入是输入矩阵的一块区域，全连接层的输入必须是一个列向量；
2.卷积层是局部连接，全连接网络使用了图像的全局信息；
全连接层和卷积层的相同点都是神经元进行点积运算。

卷积层代替全连接层的好处?
如果网络使用了全连接层，那么输入数据的尺寸一般是固定(因为全连接层的权重矩阵是固定的，
所以最后输入全连接层的feature map的尺寸也是固定的)。但是如果用卷积层代替全连接层，
就可以输入任意尺寸的数据了。
```

经典CNN模型
```text
LeNet-5,AlexNet,VGG-16,ResNet,GoogLeNet
```















### Reference(参考文档)

* [卷积理解一](https://buyi1128.github.io/2019/02/26/basicCNN/)
* [卷积理解二](https://buyi1128.github.io/2019/03/06/activate/)
* [常用数据结构和算法系列(一)](https://buyi1128.github.io/2019/03/04/basic-algorithm/)
* [常用数据结构和算法系列(二)](https://buyi1128.github.io/2019/04/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95/)
* [leetcode-github库](https://github.com/aceliuchanghong/myLeetCode)
* [CNN基础及经典模型介绍](https://zhuanlan.zhihu.com/p/344562609)
* [CNN卷积核与通道讲解](https://zhuanlan.zhihu.com/p/251068800)
